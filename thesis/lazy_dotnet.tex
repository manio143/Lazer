% Copyright (c) 2020 by Marian Dziubiak <marian.dziubiak@gmail.com>

\documentclass[en]{pracamgr}

\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{multirow}

% Dane magistranta:
\autor{Marian Dziubiak}{370784}

\title{Implementation of a lazy runtime environment on the .NET platform}
\titlepl{Implementacja leniwego środowiska uruchomieniowego na platformie .NET}

%kierunek: 
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Computer Science}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
\opiekun{dr Marcin Benke\\
  Institute of Informatics\\
  }

% miesiąc i~rok:
\date{May 2020}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
11.3 Computer Science\\ 
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{Software and its engineering\\
  Software notations and tools\\
  Compilers\\
  Runtime environments}

% Słowa kluczowe:
\keywords{functional programming, lazy evaluation, .NET CLR}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]
\newcommand{\shrp}{%
  {\fontfamily{ppl}\selectfont\#%
  }}
\newcommand{\myref}[1]{\ref{#1}.~\textit{\nameref{#1}}}

\definecolor{brown}{RGB}{216,132,24}
\definecolor{gray}{RGB}{110,110,110}
\hypersetup{
    colorlinks,
    linkcolor=brown,
    citecolor=gray,
    urlcolor=blue
}

\setcounter{secnumdepth}{3}

% koniec definicji

\begin{document}
\maketitle

\begin{abstract}
  TODO: Write a nice abstract
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

% ##########################
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
% ##########################

Functional programming is reliving a peak in its popularity.
One of the more interesting features of pure functional programming
is lazy evaluation. This means delaying the evaluation of an expression
until its value is actually needed. This allows the programmer to use
infinite data structures and certain types of algorithms that wouldn't
be easily implementable in a strict language.
Currently the main programming language
supporting lazy evaluation is Haskell.

Haskell is compiled to native code or to LLVM representation by~the~optimizing
compiler GHC. Like many other programming languages Haskell is mostly
tied to its compiler and the native platform (as opposed to a~managed platform).
Actually, Haskell did in fact have more compilers available, however,
GHC is leading the language development and left competition far behind.

There exist a couple of managed platforms that provide a common ecosystem
for multiple languages. The two most popular are JVM and .NET.
There has been some work done for the JVM \cite{Tullsen} \cite{Choi} \cite{Stewart}
and there exist two Haskell implementations: 
\href{https://github.com/Frege/frege}{Frege}~and~\href{https://eta-lang.org/}{Eta}.
In~this work I will focus on the idea of bringing
Haskell to the .NET platform.

{
    \centering \rule[3pt]{9cm}{.3pt}

}

In 2002 Microsoft has released the first version of .NET Framework with
C\shrp{} programming language. Since the initial release, the .NET ecosystem
has grown mature with many languages targeting the platform. 
Some of them are functional programming languages -- \mbox{F\shrp{}, SML.NET, Nemerle}.
In 2015 most of the platform has been open sourced and gained a lot of
interest from the community. This lead to some interesting performance
improvements.\\ \\
Previous attempts to bring
Haskell to the .NET CLR (\textit{Common Language Runtime}):
\begin{itemize}
  \item
  In 2001 Nigel Perry, Erik Meijer and Arjan van Yzendoorn implemented a 
  non-strict scripting language called Mondrian
  \cite{MondrianImplDetails}\cite{PerryMeijer}.
  
  \item
  In 2005 Monique Monteiro, Mauro Ara\'ujo, Rafael Borges and Andr\'e Santos
  created Haskell.NET
  \cite{Brazil}.

  \item
  In 2006 Oliver Hunt created a compiler from Core language to C\shrp{}, leveraging
  OOVM (\textit{Object Oriented Virtual Machine}) features to simplify interoperability
  with non-strict code from CLI languages \cite{Hunt}.
\end{itemize}

Those previous attempts did not result in widespread adoption of created solutions.
Due to the passage of time and lack of interest, it is currently impossible
to obtain the created compilers or their source code.
In order to see practical results a new compiler had to be made
and with it a new runtime focused on code execution efficiency.

\newpage
What is the reason for bringing Haskell to a managed platform?
While the performance may be lower, the generated code will be portable
and users can leverage a~variety of libraries, including the mature web framework
ASP.NET. This also goes the other way -- .NET developers could access some of
the libraries in the Hackage package repository.
Ever tried debugging Haskell code? After compiling to C\shrp{} it's possible
to setup breakpoints and analyze the state of the program (with some limitations).

\section*{Chapter contents}

\begin{itemize}
\item
Chapter \myref{r:lazyEval} introduces the desired properties of functional
languages like purity and laziness. Then it goes over a lazy program
evaluation process.
\item
Chapter \myref{r:runtime} showcases an implementation of a lazy runtime:
the representation of data, functions and computations as well as
framework classes that the compiled code uses.
\item
Chapter \myref{r:alternatives} compares this and other implementations by
showing important differences and arguing for their pros and cons.
\item
Chapter \myref{r:perf} looks at performance bottlenecks, presents
benchmark results and compares those to Haskell compilers.
\item
Chapter \myref{r:compiler} presents some aspects of the translation
mechanism from STG representation of a Haskell program into C\shrp{}.
\item
Chapter \myref{r:future} looks into future improvements to the runtime
and extending Haskell for CLI (\textit{Common Language Infrastructure}) interoperability.
\end{itemize}

% ##########################
\chapter{Laziness in functional languages}\label{r:lazyEval}
% ##########################

Functional programming languages are characterized by first
class functions and easy to model data types. Many modern
languages have functional features, but much fewer have
syntax and features that make them easy to use. In
particular, languages from the ML family such as Haskell,
Elm, OCaml and F\shrp{} have become fairly popular in the
community interested in functional programming.

\section{Desired properties of functional languages}

One of the great things about functional languages is that
they bring the program closer to mathematical notations.
Why is that a good thing? Mathematicians are able to
provide proofs for correctness of their formulas. Therefore,
an algorithm expressed in mathematical functions can be
reasoned about in a similar manner.
A great example of that is Coq -- a formal proof management
system. The proofs can be extracted into functional programs
in OCaml, Haskell or Scheme \cite{coq_extract}.

\subsection{Purity}

All functions in mathematics are \textit{pure}. However, functions
in programs can have side effects, like modifying memory or
communicating with external devices. This makes a lot of
algorithms harder to model in terms of mathematical
functions. We call a function \textit{pure} if it has no
side effects.

A pure function will always return the same result for the
same set of arguments. This means expressions that apply a
pure function to some arguments are referentially
transparent \cite{referentialTransparency} -- can be substituted by the value they would
evaluate to. Referential transparency isn't a property of
impure functions that may rely on their code getting
invoked multiple times, each time returning a different
value.

Furthermore, when dealing with just pure functions, the
compiler may easily apply an optimization called Common
Subexpression Elimination (CSE) which computes the value of
an expression once and uses it in multiple places.

\begin{verbatim}
                              f a <*> f a <*> f a
                            ----------------------
                                let x = f a in
                                x <*> x <*> x
\end{verbatim}

\subsection{Laziness}\label{s:laziness}

In mathematics it doesn't matter which part of the formula
is computed first. In a language with only pure functions,
such as Haskell, since there are no side effects that may
have to happen in a particular order, the order of
expression evaluation can be changed. In particular we can
delay the evaluation until the moment when we actually need
to know the computed value. It may turn out we don't
actually need to perform the computation. But if we do
evaluate the expression, its value should be saved so that
subsequent evaluations of that same expression can use that value.

In most strict languages there is a minimal amount of
laziness. It can be most often found in conditional expressions.
For example: if the left argument of the
boolean \texttt{\&\&} operator is false then the right argument is not
evaluated. This is especially useful when eager evaluation would lead
to an error. Higher level languages also include some sort of lazy
type (e.g. \verb|Lazy<T>|~in~C\shrp) that allows delaying computations.
However, it is more cumbersome to use and making it an explicit type
makes it harder to use in contexts where laziness was not considered.

There are a couple reasons why using lazy evaluation can be beneficial.
First of all it allows to think about certain problems in more mathematical
ways -- e.g. list of all prime numbers. Such a data structure cannot
be computed in finite memory, but it's possible to operate on it as if it
was. This is especially important when dealing with Big Data as large
amounts of information cannot be kept fully in memory, but their subset can.
This is leveraged by frameworks such as Spark and found to increase performance
of computations \cite{lazySpark}. Lazy evaluation also allows to avoid
performing expensive computations when the result is not going to be used.
If we are passing a value to an unknown function we don't know if it 
will be used. In a strict language this computation would have to
be performed either way. Finally, there exist certain data structures that
leverage lazy evaluation to provide amortization of asymptotic complexity,
e.g. queues \cite{amortized}.

\section{Representation of functional programs}

A functional program can often be expressed in terms of
nested expressions that form a tree. By rewriting the
simple expressions representing a named value into a
reference to the bound expression, we obtain an expression
graph. The process of evaluation of that program is
equivalent to the process of graph reduction of the expression graph.

An example of such a reduction is the $\beta$-reduction in lambda calculus.
Given a function application $\mathtt{f\cdot a}$ where
\texttt{f} is a lambda abstraction $\mathtt{\lambda x. e}$,
performing a $\beta$-reduction yields a new expression
\texttt{e'} that is equal to \texttt{e} with all
occurrences of \texttt{x} replaced by \texttt{a}.

Expressions are reduced to weak head normal form (WHNF)
which is either a primitve value (e.g. an integer),
a data cell (structured data, e.g. a tuple), a function
value or a non-saturated function application (not enough arguments).
For more theoretical information on evaluating expression see \cite{spj-book}.

\subsection{Practical approaches}

Graph reduction is rather simple conceptually, but it turns
out to be harder to implement efficiently. Over the
years different approaches were created to compute lazy languages: 

\newpage
\begin{itemize}
    \item combinator machines (e.g. SKIM) \cite{combinators} \cite{SKIM}

        Any program in lambda calculus can be transformed into application
        of basic combinators S and K. The SKI Machine is a description
        of instructions and hardware specific to functional program
        reduction that has been compiled into combinatorial form.

        \begin{verbatim}
    S = \f \g \x -> f x (g x)
    K = \x \y    -> x\end{verbatim}

    \item lazy SECD machine CASE \cite{SECDM} \cite{CASE}
    
        In this approach lambda expressions are reduced by following
        operational semantics of an abstract machine. CASE extends SECD
        with closures which eliminate multiple copying parts of the environment
        multiple times onto the stack.
        
        Acronyms describe machine state: \\
        \texttt<Stack, Environment, Cotrol, Dump\texttt> \\
        \texttt<Code, Argument register, Stack, Environment\texttt>
    
    \item G-Machine \cite{G-Machine} \cite{spj-book}
    
        The Graph Reduction Machine was designed as a fairly efficient
        way to evaluate lazy functional programs on existing
        hardware.

    \item Categorical Abstract Machine \cite{CAM}
    
        CAM similarly to CASE is a successor of SECD.
        However, it is based on Categorical Combinators
        \cite{categorical_combinators}.

    \item Three-Instruction-Machine (TIM) \cite{TIM}
    
        TIM replaced abstract expression graph representation
        by actual machine code to be executed.
        The computations are compiled in supercombinator form
        and executed with just three instructions: Take and Push
        (stack manipulation), and Enter.

    \item Spineless Tagless G-Machine (STGM) \cite{STGM}
    
        An extension of the G-Machine that discarded spines
        (chains of thunk references) and tags (data structure information).
        Instead it treated everything equally as a closure
        whose code was executed on entry.

\end{itemize}

The Glasgow Haskell Compiler (GHC) uses the STGM approach with some additional
optimizations accumulated over the years.
The STG language extends simple lambda calculus with a few
constructs. In particular, it defines a \texttt{let..in} expression
for binding expressions to identifiers and delaying their evaluation,
and a \texttt{case..of} expression for forcing expressions (evaluating them to WHNF) and pattern matching.

\subsection{Closures and Thunks}\label{s:closures}

Let's introduce the concept of a \textit{closure}.
Every lambda expression containing free variables is represented by a closure --
an object in memory holding references or values of those free variables
as well as a pointer to the code that evaluates this expression.

A closure can represent either a computation or
a function that would be applied to some arguments.
Even data can be represented by a closure, with its code
being a simple expression returning this data.

Section \myref{s:laziness} required lazy computations to
save the computed value and return it on all future
evaluations. A closure that represents an updatable
computations is called a \textit{thunk}.
Most \texttt{let} expressions create thunks.

After analyzing performance of lazy programs research shows that
many algorithms are actually faster if they are evaluated eagerly.
Therefore a process called strictness analysis is
performed by the compiler to decide whether a value should be
evaluated lazily or eagerly \cite{demand analysis}.
Therefore, there are cases when a \texttt{let} expression
may actually evaluate its subexpression instead of creating a thunk for it.

\subsection{Example evaluation process}\label{s:example_eval}

To make sure the reader can better understand the
evaluation process a simple example is presented.
The Haskell code below is written very close to its STG representation.
It shows a \texttt{Maybe} data type with a monadic \texttt{bind} operation,
a source computation \texttt{divide} and a modifying
computation \texttt{add n} represented in terms of bind.

\begin{verbatim}
    data Maybe a = Nothing | Just a
    bind f m = case m of
                Just x -> f x
                Nothing -> Nothing
    divide x y =
        case y of
            0 -> Nothing
            _ -> let v = x / y
                 in Just v
    add n = let f = \x -> let v = x + n 
                          in Just v
            in bind f
    main = let d = divide 4 2
           in add 5 d
\end{verbatim}

The steps below represent evaluation of \texttt{main}:

\begin{center}
\begin{tabular}{r | l | c | c }
    Code & Action & Heap & Stack \\
    \hline
    \hline
    \texttt{main} & create thunk \texttt{d\{divide 4 2\}} &  & return \texttt{main} \\
    \hline
    \texttt{main} & apply \texttt{add} to \texttt{5} & \texttt{d\{divide 4 2\}} & return \texttt{main} \\
    \hline
    \texttt{add} & create function \texttt{f} & \texttt{d\{divide 4 2\}} & apply in \texttt{main} \\
    & capture \texttt{n = 5} & & return \texttt{main} \\
    \hline
    \texttt{add} & return partial application \texttt{bind f} & \texttt{d\{divide 4 2\}} & apply in \texttt{main} \\
    & & \texttt{f[n = 5]\{\textit{fun}\}} & return \texttt{main} \\
    \hline
    \texttt{main} & apply \texttt{bind f} to \texttt{d} & \texttt{d\{divide 4 2\}} & return \texttt{main} \\
    & & \texttt{f[n = 5]\{\textit{fun}\}} & \\
    \hline
    \texttt{bind} & evaluate argument \texttt{m} $\rightarrow$ \texttt{d} & \texttt{d\{divide 4 2\}} & return \texttt{main} \\
    & & \texttt{f[n = 5]\{\textit{fun}\}} & \\
    \hline
    \texttt{d} & apply \texttt{divide} to \texttt{4} and \texttt{2} & \texttt{d\{\textit{evaluating}\}} & case in \texttt{bind} \\
    & & \texttt{f[n = 5]\{\textit{fun}\}} & return \texttt{main} \\
    \hline
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{r | l | c | c }
    \hline
    \texttt{divide} & evaluate argument \texttt{y} $\rightarrow$ \texttt{2} & \texttt{d\{\textit{evaluating}\}} & update \texttt{d} \\
    & check if \texttt{2} is \texttt{0} & \texttt{f[n = 5]\{\textit{fun}\}} & case in \texttt{bind} \\
    & create thunk \texttt{v\{4 / 2\}}& \texttt{v\{4 / 2\}} & return \texttt{main} \\
    & return \texttt{Just v} & & \\
    \hline
    \texttt{bind} & check if the result is \texttt{Just} & \texttt{d\{Just v\}} & return \texttt{main} \\
    & take out \texttt{x} $\rightarrow$ \texttt{v} from \texttt{d} & \texttt{f[n = 5]\{\textit{fun}\}} & \\
    & & \texttt{v\{4 / 2\}} & \\
    \hline
    \texttt{bind} & apply \texttt{f} to \texttt{x} $\rightarrow$ \texttt{v} & \texttt{f[n = 5]\{\textit{fun}\}} & return \texttt{main} \\
    & & \texttt{v\{4 / 2\}} & \\
    \hline
    \texttt{f} & create thunk \texttt{v'\{v + 5\}} & \texttt{v\{4 / 2\}} & return \texttt{main} \\
    & return \texttt{Just v'}& \texttt{v'\{v + 5\}} & \\
    \hline
\end{tabular}
\end{center}

If we now unpack \texttt{Just v'} and evaluate \texttt{v'} we will obtain the value \texttt{7}.


% ##########################
\chapter{Lazy runtime}\label{r:runtime}
% ##########################

This chapter presents an implementation of a lazy runtime environment
on the .NET platform.
The runtime is called \textit{Lazer} inspired by Razor and Blazor,
web page frontend scripting systems that use C\shrp{}.
Lazer has been implemented in C\shrp{} with some
extensions to allow emitting certain Common Intermediate Language (CIL)
instructions not available in the regular compiler.

The implementation is best explained by following the example code execution
from chapter \myref{s:example_eval}.
First, let's introduce some key concepts regarding data representation
and then move into computation representation.

\section{Types and data representation}\label{s:data_rep}

Types can be divided into \textit{lifted} and \textit{unlifted}.
Lifted types include $\bot$ (bottom) as a possible value.
Actually, $\bot$ represents an infinite computation or
a program error. Unlifted types on the other hand will
always represent a proper value.

Unlifted types such as \texttt{Int\#}, \texttt{Double\#}
and \texttt{Char\#} can be represented in C\shrp{} by
primitive types like \texttt{int}/\texttt{long}, \texttt{double} and \texttt{char}.
Other unlifted types include tuples \texttt{(\#,\#)} and
arrays \texttt{ByteArray\#}, which can be represented by
\textit{structs} (.NET value types) and arrays (\texttt{byte[]}).

Lifted types will be represented by a pointer reference.
For example an instance of a lifted type \texttt{Int} is an object representing
its data constructor \texttt{I\#} that has one field of type \texttt{Int\#}.
In practice this means data constructors represent .NET classes.

\begin{verbatim}
    public sealed class I : Data {
        public int x0;
        public I(int x0) {
            this.x0 = x0;
        }
    }
\end{verbatim}

Haskell's type system cannot be represented in C\shrp{} directly.
One of the more advanced Haskell features are higher-kinded types (HKT).
This allows type constructors to take polymorphic parameters
that are also type constructors, i.e.:

\begin{verbatim}
    newtype IdentityT f a = IdentityT { runIdentityT :: f a }
\end{verbatim}

Here \texttt{f} is a type variable representing a type constructor
and is later applied to the type variable \texttt{a}.
The .NET runtime does not allow for generic type parameters to be
type constructors. In other words .NET's generic type parameters are
equivalent to Haskell's type variables of kind \texttt{*}.
So types like \texttt{IdentityT} are not
expressible in C\shrp{} without type erasure or \textit{implementation leakage}.
The user of \texttt{IdentityT} would have to know how it
was defined and pass it the applied type \texttt{f a}. This can be
problematic -- especially when type arguments are used as an abstraction layer.
Therefore this runtime ignores types, by
means of type erasure.

\subsection{Data superclass}

Data constructors implemented in C\shrp{} like \texttt{I} defined above
will share some common properties. Thus, they will extend the \texttt{Data} class.
As mentioned in \myref{s:closures}, data constructors are
also closures, but very simple ones, having a simple
\texttt{return this;} in their evaluation method.

\begin{verbatim}
    public abstract partial class Data : Closure
    {
        public override Closure Eval() => this;
        public override int Tag => 1;
    }
\end{verbatim}

The \texttt{Data} class also provides a default \texttt{Tag}
value so that single constructor types don't have to override it.

\subsection{Data constructor definition}

All data constructors fit a simple template.
Fields in the constructor are all public, ordered and named accordingly
(\texttt{x0}, \texttt{x1}, \texttt{x2}, ...).
The constructor's class has a \textit{constructor} that takes
as many arguments as there are fields.
The class is sealed (cannot be extended) so that efficient
type checks can be performed.

\begin{verbatim}
    public sealed class [Name] : Data {
        public [Type0] x0;
        ...
        public [TypeN] xN;

        public [Name]([Type0] x0, ..., [TypeN] xN) {
            this.x0 = x0;
            ...
            this.xN = xN;
        }

        public override int Tag => [Tag]
    }
\end{verbatim}

The \verb|[Type]| is either an unlifted value type
(e.g. \texttt{int}, \texttt{double}) or a pointer reference
of type \texttt{Closure}. Why \texttt{Closure} and not \texttt{Data}?
Because, data constructors can be applied to unevaluated computations.

If a type has more than one constructor, each one has to
override the \texttt{Tag} property with an integer value.
This value is the constructor's index in the type definition of
the Haskell program and goes from 1 to $n$.
Tag can be used for \texttt{case} expressions explained in \myref{s:compute_cond}.

\section{Computation representation}\label{s:computations}

In C\shrp{} a unit of computation is a method.
There are two kinds of methods:
\begin{itemize}
    \item instance methods -- they are bound to objects and may be overloaded,
    \item static methods -- they are bound to types and globally accessible.
\end{itemize}
This leads to two ways of implementing closures.
Either a closure is represented by an instance of a class with an
overloaded instance method or by an instance of a predefined universal
class that is initialized with a reference to a static method.

The first approach means that for every computation a type has to be created.
Those types are loaded by the .NET runtime before first use and incur a small
cost of initialization. With a huge amount of types this may visibly impact performance.
There are additional problems with function application techniques.
Therefore, the second approach has been chosen for the Lazer runtime.

\subsection{Computation as a static method}

Every computation is represented by a static method that follows a simple template:

\begin{verbatim}
    public static [RetType] [Name]_Entry(
        [FreeType0] [FreeName0], ..., [FreeTypeK] [FreeNameK],
        [ArgsType0] [ArgsName0], ..., [ArgsTypeN] [ArgsNameN]) {
            [Expression]
        }
\end{verbatim}

The \verb|[RetType]| is the return type of the method,
equivalent to the type of \verb|[Expression]|
(for all lifted types it's \texttt{Closure}).
The \verb|[Name]| is the bound identifier of the computation.
The first $k$ arguments are the free variables of the computation.
The following $n$ arguments are formal parameters, if the computation represents a function.

\subsection{Computing a data cell}\label{s:compute_data}

Let's look at the \texttt{divide} function from \myref{s:example_eval}.
It evaluates its second argument and performs a value check on it.
Then it delays the actual division (a costly operation) and returns a new data cell.
This can be represented in C\shrp{} by the following methods:

\begin{verbatim}
    public static Closure divide_Entry(Closure x, Closure y) {
        var yI = y.Eval() as I;
        var yVal = yI.x0;
        switch (yVal) {
            case 0:
                return new Nothing();
            default:
                var v = new Updatable<Closure, int>(divide_v_Entry, x, yVal);
                return new Just(v);
        }
    }
    public static Closure divide_v_Entry(Closure x, int yVal)   {
        var xI = x.Eval() as I;
        var xVal = xI.x0;
        var d = xVal / yVal;
        return new I(d);
    }
\end{verbatim}

Creating a new heap allocated object ––~a data cell~--
is easy in C\shrp{}. In STG all constructor applications are
saturated so we don't have to deal with an
insufficient number of constructor arguments. Here we can
also see that \texttt{v} is not a data value, but a delayed,
updatable computation. When \texttt{Eval} method is called
on \texttt{v}, the \verb|divide_v_Entry| method will be
invoked with captured values \texttt{x} and \texttt{yVal}.

\subsection{Computing a function}

Let's look at the \texttt{add n} function from \myref{s:example_eval}.
It creates a new function \texttt{f} that captures argument \texttt{n}
and partially applies function \texttt{bind} to the created function.
This can be represented in C\shrp{} by the following methods:

\begin{verbatim}
    public static Function bind = new Fun(2, bind_Entry);
    ...
    public static Closure add_Entry(Closure n) {
        var f = new Fun<Closure>(1, add_f_Entry, n);
        return bind.Apply<Closure,Closure>(f);
    }
    public static Closure add_f_Entry(Closure n, Closure x) {
        var v = new Updatable<Closure, Closure>(Int_add, x, n);
        return new Just(v);
    }
\end{verbatim}

The \texttt{Fun} class takes a reference to the computation method
and the function's arity -- number of arguments.
It can also capture free variables,
as is the case here when we pass \texttt{n} to the constructor.
The \texttt{Apply} method checks the number of provided
arguments and if saturated performs the computation.
Otherwise it returns a partial application object (PAP).

\subsection{Performing conditional computations}\label{s:compute_cond}

Let's look at the \texttt{bind f m} function from \myref{s:example_eval}.
It performs pattern matching on its second argument.
An alternative computation is performed based on the satisfied condition.
This can be represented in C\shrp{} by the following method:

\begin{verbatim}
    public static Closure bind_Entry(Closure f, Closure m) {
        m = m.Eval();
        switch (m) {
            default: 
                throw new ImpossibleException();
            case Nothing m_Nothing:
                return new Nothing();
            case Just m_Just:
                var x = m_Just.x0;
                return f.Apply<Closure,Closure>(x);
        }
    }
\end{verbatim}

We represent STG \texttt{case} expressions in C\shrp{} with
\texttt{switch} statements. An example of this has already
been shown in \myref{s:compute_data} when checking if an
argument had value \texttt{0}. While switching on primitive
values is simple, it's a little more complex with data constructors.

To get the arguments of a data constructor, the object has
to be cast from the general type \texttt{Closure} to the
specific type of the constructor.
The .NET Runtime is type safe in that it performs type
checks on casts to validate data consistency.
The most efficient cast available is an \texttt{isinst} CIL
instruction that applied on a sealed type results in four ASM instructions:

\begin{enumerate}
    \item get pointer to object's MethodTable,
    \item compare it to the sealed type's MethodTable pointer
    \item jump forward one instruction if pointers are equal
    \item otherwise assign \texttt{null} to the variable 
\end{enumerate}

There are two ways to perform constructor choice in a switch statement:
by tag or by type. Doing a switch on type performs the type
cast and choice simultaneously but it is linear to the
number of alternatives. Doing a switch by tag still
requires the type cast, but for more than 2 constructors (3
branches including the \texttt{default}) it is implemented efficiently as a jump table.
Tests in \myref{perf:switching} show that switching on tag
becomes beneficial with switches of 5 or more alternatives.

Enumerator types are types whose every data constructor has no arguments.
Switching on those is always performed by tag, because no casts are needed.

In the \texttt{default} branch of the example an exception is thrown that marks
an impossible branch.
The reason why it had to be included is because C\shrp{}
cannot tell that no other values are possible.
That's actually not due to type erasure -- any switch statement that
is the last statement in a function has to have a default case.

\section{Runtime implementation}\label{s:runtime_impl}

The lazy runtime is a set of abstractions and tools
that allow for efficient execution of lazy code.
This section presents the details of what has been implemented,
how it works, and why it is efficient.

\subsection{Abstract Closure class}

All heap allocated objects: computations, lifted values,
and functions share a common superclass –– the \texttt{Closure}.
Closure provides three distinct interfaces for each of the aforementioned object types:

\begin{enumerate}
    \item \texttt{Eval()} method -- computations are
    invoked and expressions evaluated to WHNF, resulting in
    the returned value. For data and functions there is
    nothing to compute as they are already in WHNF so they
    can just return themselves.
    \item \texttt{Tag} property -- allows to switch on data
    constructors without a cast beforehand. Functions and
    computations throw an exception if used.
    \item \texttt{Apply(...)} methods -- used to apply
    functions to their arguments. Data values cannot be
    applied and throw an exception. Computations are first
    evaluated and then the result is applied.
\end{enumerate}

\begin{verbatim}
    public abstract class Closure {
        public abstract Closure Eval();
        public abstract     int Tag { get; }
        public abstract       R Apply<A0,R>(A0 a0);
        public abstract       R Apply<A0,A1,R>(A0 a0, A1 a1);
        ...
    }
\end{verbatim}

Every method in C\shrp{} has a fixed number of parameters (no varargs).
This means the runtime establishes maximal arity of functions
(with the number of \texttt{Apply} methods).
Thanks to currying any function of greater arity can be
rewritten to return a function of smaller arity \cite{Curry}.
Furthermore, \texttt{Apply} methods are generic to achieve
the most flexibility and succinctness.
This in turn means type parameters need to be provided at every call site.

\subsection{Abstract Computation and Data classes}

From the previous section follow the definitions of  abstract \texttt{Computation} and \texttt{Data} classes:

\begin{verbatim}
    public abstract class Computation : Closure
    {
        public override int Tag
            => throw new NotSupportedException(
                            "Accessing Tag on a computation.");

        public override R Apply<A0, R>(A0 a0)
            => this.Eval().Apply<A0, R>(a0);

        public override R Apply<A0, A1, R>(A0 a0, A1 a1)
            => this.Eval().Apply<A0, A1, R>(a0, a1);
        ...
    }
\end{verbatim}
\begin{verbatim}
    public abstract class Data : Closure
    {
        public override Closure Eval() => this;
        public override int Tag => 1;

        public override R Apply<A0, R>(A0 a0)
            => throw new NotSupportedException(
                            $"Cannot apply a value ({GetType()})");

        public override R Apply<A0, A1, R>(A0 a0, A1 a1)
            => throw new NotSupportedException(
                            $"Cannot apply a value ({GetType()})");
        ...
    }
\end{verbatim}

\newpage
\subsection{Abstract Function class}\label{s:function_class}

While previous abstractions were pretty straight forward,
functions are a bit more complicated. As mentioned before,
C\shrp{} methods have to be invoked with all of the
parameters they require.
This means functions have to provide an additional check of arity.
There are three possible situations:

\begin{enumerate}
    \item function is applied to the exact number of arguments
    -- invoke the computation method;
    \item function is applied to too many arguments
    -- apply the function to the exact number of arguments
    and then apply the result to the rest;
    \item function is applied to too few arguments 
    -- create a partial application object.
\end{enumerate}

\begin{verbatim}
    public abstract partial class Function : Closure
    {
        public override Closure Eval() => this;

        public override int Tag
            => throw new NotSupportedException(
                            "Accessing Tag on a function.");
        
        public abstract int Arity { get; }
            
        public abstract R ApplyImpl<A0, R>(A0 a0);
        public abstract R ApplyImpl<A0, A1, R>(A0 a0, A1 a1);
        ...
    }
\end{verbatim}

The above part of the \texttt{Function} class implements \texttt{Eval}
and \texttt{Tag} inherited from \texttt{Closure} and introduces
new members: \texttt{Arity} property and \texttt{ApplyImpl} methods.
Below is one of the overloaded \texttt{Apply} methods that shows
the three situations outlined above:

\begin{verbatim}
    public override R Apply<A0, A1, R>(A0 a0, A1 a1)
    {
        Closure h;
        switch (this.Arity)
        {
            case 1:
                // too few arguments
                h = this.ApplyImpl<A0, Closure>(a0);
                return h.Apply<A1, R>(a1);
            case 2:
                // exact number of arguments
                return this.ApplyImpl<A0, A1, R>(a0, a1);
            default:
                // too many arguments
                return (R)(object)new PAP<A0, A1>(this, a0, a1);
        }
    }
\end{verbatim}

The partial application object holds a reference to the
applied function and captured arguments.
When applied it applies the original function to all available arguments.

\begin{verbatim}
    public abstract class PAP : Closure
    {
        public Function f;
        protected PAP(Function f) => this.f = f;

        public override Closure Eval() => this;

        public override int Tag
            => throw new NotSupportedException(
                            "Accessing Tag on a PAP.");
    }
\end{verbatim}
\begin{verbatim}
    public sealed class PAP<B0> : PAP
    {
        public B0 x0;
        public PAP(Function f, B0 x0) : base(f)
            => this.x0 = x0;

        public override R Apply<A0, R>(A0 a0)
            => f.Apply<B0, A0, R>(x0, a0);
        
        public override R Apply<A0, A1, R>(A0 a0, A1 a1) 
            => f.Apply<B0, A0, A1, R>(x0, a0, a1);
        
        public override R Apply<A0, A1, A2, R>(A0 a0, A1 a1, A2 a2)
            => throw new NotSupportedException(
                            "Application exceeds runtime argument limit.");
    }
\end{verbatim}

With this implementation PAPs never form a chain of objects, but rather return a new ``flat'' PAP when applied to too few arguments.

\subsection{Abstract Thunk class}\label{s:thunk_class}

In \myref{s:closures} a \textit{thunk} has been defined as a
closure that updates itself after it is evaluated.
The \texttt{Thunk} class is going to extend
\texttt{Computation} class and provide this update
mechanism. Upon invoking the \texttt{Eval} method the first
time, the computation is performed and the result saved.
The thunk now becomes an indirection and on future evaluations
the the saved result is returned.

This can be implemented in two ways. One is to perform a comparison
of the result pointer with \texttt{null} to see if
the computation has been performed.
The other one involves modifying the method that is called
to perform evaluation.
Performance tests show that on the .NET platform the first approach is more efficient
due to a more expensive constructor call for every thunk
(see \myref{perf:thunk_ind} for more details).

\newpage
\begin{verbatim}
    public abstract class Thunk : Computation
    {
        public Closure ind;

        protected abstract Closure Compute();
        protected virtual void Cleanup() { }
        
        public override Closure Eval()
        {
            if (ind != null) 
                return ind.Eval();

            ind = Blackhole.Instance;
            ind = Compute();
            Cleanup();
            return ind;
        }
    }
\end{verbatim}

The \texttt{Compute} method is implemented in subclasses
and performs the actual computation.
It's result is saved in the \texttt{ind} field.
But during the computation \texttt{ind} is assigned a special
value called the black hole that allows detecting
invalid self-references (computation loops).
If the computation of thunk tries to evaluate itself
again it will end up evaluating the black hole, which stops
the program. Black holes can
be further used to implement synchronization in
multithreaded contexts \cite{multiprocessor}.
However, the currently used implementation is extremely simple:

\begin{verbatim}
    public sealed class Blackhole : Computation
    {
        public override Closure Eval() =>
            throw new System.Exception("BLACKHOLE");
        public static Blackhole Instance = new Blackhole();
    }
\end{verbatim}

For explanation on the \texttt{Cleanup} method, see \myref{s:updatables}.

\subsection{CLR intrinsic functions}

The user code examples shown in \myref{s:computations}
have been simplified for clarity. However,
whenever a reference to a method is mentioned, it is
understood as a pointer to the method's entry point.

The .NET's Common Language Runtime (CLR) provides primitives
that make it possible to obtain a pointer to a method
and to later call this method indirectly via that pointer.
Unfortunately, the C\shrp{} language doesn't provide means to
utilize those features. As such, a modified version of the compiler
had to be used that includes a sort of a hack that
enables compiling C\shrp{} code with those CLR intrinsic instructions.

Placing \texttt{CompilerIntrinsicAttribute} on a 
\texttt{static unsafe extern} method named after
the desired instruction tells the modified compiler to emit the
instruction rather then a method call.
This runtime uses the following two functions:

\begin{itemize}
    \item \texttt{LoadFunctionPointer} -- represents the 
    \texttt{ldftn} instruction and takes a method group as a parameter;
    \item \texttt{TailCallIndirectGeneric} -- represents a 
    call to a generated method whose body is composed of a \texttt{calli} 
    instruction with a \texttt{.tail} modifier --
    this method takes arguments followed by the function pointer
    to be called with those arguments.
\end{itemize}

The C\shrp{} language provides a type safe mechanism for manipulating
functions -- delegates. However, due to the overhead of
additional allocations and checks on delegate creation they
don't seem to be a good fit for an efficient runtime implementation. For more details see \myref{perf:fun_pointers}.

\begin{verbatim}
    [CompilerIntrinsic]
    public static unsafe extern void*
        LoadFunctionPointer<T1, T2>(Func<T1, T2> fun);

    [CompilerIntrinsic]
    public static unsafe extern U
        TailCallIndirectGeneric<T0, U>(T0 x0, void* funPtr);
\end{verbatim}

It is worth mentioning that the RyuJIT, which generates
machine code during execution of a .NET program,
can in fact optimize method calls in tail position
to proper tail calls, even without the \texttt{.tail}
modifier. In order to do so, the program has to be compiled
in Release mode or with the optimization flag enabled.

\subsection{Universal classes}

In order to limit the amount of types, rather than having
a type per closure, a different approach has been chosen.
Universal types take a function pointer and make
indirect calls to it.
They also have generic type parameters to allow creating
type safe and efficient instances with customized fields.

% point to performance difference

\subsubsection{Updatable class family}\label{s:updatables}

Section \myref{s:compute_data} presented usage of an Updatable class.
It is a subclass of \texttt{Thunk} that takes a function
pointer and free variables. When evaluated, it performs an
indirect call to the function pointer with those free variables as arguments.

There is an \texttt{Updatable} class per number of free variables. The types of free variables are provided in generic parameters. Below is an example of such a class:

\begin{verbatim}
    public unsafe class Updatable<T0> : Thunk
    {
        protected internal void* f;
        public T0 x0;

        public Updatable(void* f, T0 x0)
        {
            this.f = f;
            this.x0 = x0;
        }

        protected override Closure Compute()
            => CLR.TailCallIndirectGeneric<T0, Closure>(x0, f);
        
        protected internal override void Cleanup()
            => this.x0 = default;
    }
\end{verbatim}

The \texttt{Cleanup} method is responsible for freeing
any references to other heap allocated objects, so that
they can be collected by the Garbage Collector (GC).
Cleanup is invoked after the computation has finished
and those free variables are no longer needed.
It's currently not possible to only set object references to null,
but all values are zeroed, because of the generic nature
of the class.

\subsubsection{Fun class family}

Similarly to \texttt{Updatable} classes the \texttt{Fun}
classes take a function pointer and free variables.
They also take the arity of the function.
Below is an example of such a class:

\begin{verbatim}
    public unsafe class Fun<F0> : Fun
    {
        public F0 x0;

        public Fun(int arity, void* f, F0 x0) : base(arity, f)
            => this.x0 = x0;

        public override R ApplyImpl<A0, R>(A0 a0)
            => CLR.TailCallIndirectGeneric<F0, A0, R>(x0, a0, f);

        public override R ApplyImpl<A0, A1, R>(A0 a0, A1 a1)
            => CLR.TailCallIndirectGeneric<F0, A0, A1, R>(x0, a0, a1, f);
        ...
    }
\end{verbatim}

The base \texttt{Fun} class without free variables declares
the protected fields for the function pointer and arity.
As was described in section \myref{s:function_class},
the universal classes provide all the \texttt{ApplyImpl} methods,
but depending on the \texttt{Arity} property,
only the one with the exact number of arguments may be invoked.

\subsubsection{SingleEntry class family}

Thanks to the demand analysis \cite{demand analysis} the compiler can
tell that some thunks are evaluated just once.
In that case it's not necessary for them to perform updates.
For example, list concatenation \texttt{(++)} only evaluates its
second argument once, when returning it if the first argument is \texttt{[]},
therefore you can pass a one time computation to it.

\begin{verbatim}
    (++) x y = case x of
                [] -> y    -- return y.Eval();
                (x:xs) -> let u = xs ++ y in x : u
\end{verbatim}

A thunk-like one time computation is called a \textit{single entry thunk}.
A~\texttt{SingleEntry} takes a function pointer and free
variables. When evaluated it makes an indirect call to the
function pointer with free variables as arguments.
Upon returning the result the \texttt{SingleEntry} object
is free to be collected by the GC.
Below is an example of such a class:

\begin{verbatim}
    public unsafe class SingleEntry<T0> : Computation
    {
        protected internal void* f;
        public T0 x0;

        public SingleEntry(void* f, T0 x0)
        {
            this.f = f;
            this.x0 = x0;
        }

        public override Closure Eval()
            => CLR.TailCallIndirectGeneric<T0, Closure>(x0, f);
    }
\end{verbatim}

\subsubsection{Number of universal classes}

How many classes is enough? It is a hard question, because Haskell
programs have no limitation on the number of free variables in an
expression. Same goes for number of function arguments.
An approximation based on a few sample programs can be made.
In particular, compiling the Prelude, that is Haskell's standard
library, should give a good view of most Haskell programs.

In my efforts of compiling Prelude, described below, I came across
functions with up to 8 free variables and up to 6 arguments to apply to.
For thunks it was 11 free variables.

In order to simplify work on the runtime, and because all those classes
are quite similar, it was possible to write a small class generating script.
It takes the number of free variables and function arguments and renders
the bodies of all universal classes, as well as \texttt{Apply} methods
of abstract classes and the CLR instrinsic methods.

\section{Standard library}

One of the goals of this project is to allow compiling
Haskell code to run on Lazer, the lazy .NET runtime.
Majority of existing Haskell code uses its Prelude library.
It includes primitive operations (e.g. manipulating byte arrays),
arithmetic operations, popular type classes,
IO operations, system interaction, and data manipulation
(lists, character strings, optional \texttt{Maybe}, etc).

With the experimental compiler described in \myref{r:compiler}
it was possible to compile a very small part of Prelude that allowed
running some basic Haskell programs. Compiled modules include

\begin{itemize}
    \item \texttt{GHC.Prim} - primitive operations like throwing exceptions,
            manipulating arrays, and sequencing evaluation. Many
            operations described in Haskell's \texttt{GHC.Prim} are accessed
            through other means, native to the .NET platform.
    \item \texttt{GHC.Types} - basic lifted types: \texttt{Int}, \texttt{Word},
            \texttt{Char}, \texttt{Float}, \texttt{Double}, \texttt{Bool},
            \texttt{Ordering}, \texttt{List}.
    \item \texttt{GHC.Tuple} - tuple types.
    \item \texttt{GHC.CString} - conversion between .NET's \texttt{string} and
            Haskell's \texttt{[Char]}.
    \item \texttt{GHC.Classes} - \texttt{Eq} and \texttt{Ord} type classes with
            instances for basic types.
    \item \texttt{GHC.Maybe} - the optional type with \texttt{Eq} and \texttt{Ord} instances.
    \item \texttt{GHC.Num} - the \texttt{Num} type class with instances for \texttt{Int},
            \texttt{Word} and \texttt{Integer}.
    \item \texttt{GHC.List} - list operations.
    \item \texttt{GHC.Base} - most common operations and type classes.
    \item \texttt{GHC.Integer.Type} - arbitrary precision integer arithmetics.
\end{itemize}

Some parts that aren't supported yet had to be cut out from those.
In general compiling the base package turned out to be much harder
then initially expected. There is complex integration between the GHC compiler and the
base library as well as holes in their documentation.
Nonetheless, the achieved results are good enough to be usable in benchmarks
(see \myref{s:benchmarks}).

It should be noted that the .NET runtime comes with its own
rich standard library. Further investigation needs to be done
to see when it can be utilized.

% ##########################
\chapter{Comparison to related work}\label{r:alternatives}
% ##########################

The development of the lazy runtime was influenced by a
number of existing solutions. However, most papers on the
topic did not provide publicly available source code
material, or such source code is no longer available because
of time passage. This makes it a bit harder
to reason about practical efficiency of those solutions.

First, we'll look at the work done for the Java Virtual Machine (JVM),
especially the very robust GHC port called Eta.
Then we'll compare the lazy runtime to other .NET based works.

\section{JVM based solutions}

In 2007 Brian Alliet wrote a paper \cite{Alliet} in which
he pointed out previous work done on the subject of
translating Haskell to Java and suggested possible improvements.
It mentions the works of Mark Tullsen \cite{Tullsen},
Kwanghoon Choi et al. \cite{Choi}
and Don Stewart \cite{Stewart} and their key differences.
It also points out that the use of \textit{eval/apply} application method
may be better suiting for the Object Oriented Virtual Machine (OOVM)
then the previously popular \textit{push/enter} method
\cite{fastcurry}.

One of the biggest challenges on the JVM is the lack of
tail calls. For most functional recursive programs this
is a huge obstacle. The solution to this problem
usually comes down to implementing an iterative interpreter
that substitutes recursion.
Potentially, a modified version of the JVM supporting tail calls
could be used, but this lowers the \textit{portability} of the
compiled program as it now has to be shipped with its runtime.
The .NET platform supports tail calls out of the box
(at least in 64-bit environments).

Moreover, there is no efficient mechanism similar
to CLR's function pointers in the JVM.
Instead more emphasis is placed on passing around objects
that implement a particular interface.
This means the first approach described in \myref{s:computations}
has to be used.
Generics on the JVM are also \textit{weaker} in that they don't
support primitive types.

There is, however, a somewhat successful implementation of
Haskell (particularly GHC Haskell) on the JVM -- the Eta
platform developed by Typelead. Unfortunately, there seem
to be no papers related to its development, but it is
open source and available on GitHub.

\subsection{Eta}

Eta is presented as a variant of Haskell. It is based on
GHC 7.10.3. The project is composed of a GHC fork with
modified backend and additional tooling for building
and managing packages.

One of the key features of Eta is its Foreign Function Interface (FFI)
that allows to export Haskell functions as Java methods
as well as to import and use Java objects.
The latter is done by providing a special \texttt{Class} type class
that can be evaluated in the \texttt{IO} monad.

The Eta runtime is much more complex than this lazy runtime.
It provides advanced features for IO, managing threads, mutable variables,
selector thunks, Software Transactional Memory, and more.
But it is still limited by the JVM and the aforementioned
constraints. This leads to more work involved in porting
Haskell libraries as special trampolines have to be used
to mark recursive functions.

Let's compare the way a \texttt{map} function would be compiled
on Eta and this lazy runtime:

\begin{verbatim}
    // Eta_map.java
    public static class Map extends Function2 {
      public static Map INSTANCE = new Map();

      public static Closure call(StgContext ctx, Closure f, Closure l) {
         Closure l_ = l.evaluate(ctx);
         if (l_ instanceof Nil) {
            return Types.NilInstance();
         } else {
            Cons cons = (Cons)l_;
            MapThunk tail = new MapThunk(f, cons.x2);
            Ap2Upd head = new Ap2Upd(f, cons.x1);
            return new Cons(head, tail);
         }
      }

      public final Closure apply2(StgContext ctx, Closure f, Closure l) {
         if (ctx.trampoline) {
            Stg.apply2Tail(ctx, this, f, l);
         }
         return call(ctx, f, l);
      }
      public final Closure enter(StgContext ctx) {
         if (ctx.trampoline) {
            Stg.enterTail(ctx, this);
         }
         return call(ctx, ctx.R1, ctx.R2);
      }
    }
\end{verbatim}

The \texttt{map} function is represented by a class deriving
from \texttt{Function2} -- a function of arity 2.
The computation is described by the static method \texttt{call}
that evaluates its second argument and checks which data
constructor it is. If it is \texttt{Cons} then it
allocates a new thunk that applies \texttt{map} on the function and tail, another thunk that applies the
function on the head, and returns a new data cell.

Then two instance methods are shown: \texttt{apply2} and \texttt{enter}.
Eta supports both \textit{eval/apply} and \textit{push/enter}
function application methods.
Both of those functions check if the computation is
executed on a trampoline in which case they have to
perform a `bounce' (unroll the stack)
and then call the static method \texttt{call}.

\begin{verbatim}
    // Lazer_map.cs
    public static Function map = 
        new Fun(2, CLR.LoadFunctionPointer(map_Entry));

    public static Closure map_Entry(Closure f, Closure l)
    {
        var l_ = l.Eval();
        switch (l_)
        {
            default: { throw new ImpossibleException(); }
            case Nil l_Nil: { return nil; }
            case Cons l_Cons:
                {
                    var h = l_Cons.x0;
                    var t = l_Cons.x1;
                    var tail = Updatable.Make(
                        CLR.LoadFunctionPointer(map_Entry), f, t);
                    var head = Updatable.Make(
                        CLR.LoadFunctionPointer(map_head_Entry), f, h);
                    return new Cons(head, tail);
                }
        }
    }
    public static Closure map_head_Entry(Closure f, Closure h)
    {
        return f.Apply<Closure,Closure>(h);
    }
\end{verbatim}

The main difference is that there is no special class for map,
but rather a universal class \texttt{Fun} is instantiated
with the pointer to the static computation method.
The same goes for the thunks created in the Cons branch.
Thanks to tail calls no trampolines are involved, which
increases performance.

The \textit{eval/apply} model has been implemented a bit differently
in Eta in that there are abstract subclasses \texttt{FunctionN}
of the base class \texttt{Function} that implement all
apply methods except \texttt{applyN} that is implemented
by the actual function implementation.
Those already implemented methods take care of creating
PAPs and further result applications.

\section{Mondrian}

Mondrian was a lazy functional scripting language for
the .NET platform. It was developed during early access
to the first version of the runtime.
It was the first approach to bring non-strict
semantics to .NET.

Two papers have been discovered that
describe Mondrian. The first one \cite{MondrianImplDetails}
introduces the language and provides its implementation details.
The second paper \cite{PerryMeijer} discusses more generally
the implementation of Mondrian and other lazy languages
for both .NET and JVM (OOVMs in general).
Mondrian is a separate language from Haskell and has
been designed for interoperability with other .NET languages.

Let's take an overview look on its implementation.
There is no common base class (except for \texttt{Object}).
Data types are POCO (plain old CLR object) and functions
implement the following interface:
\begin{verbatim}
    interface Code {
        public Object ENTER();
    }
\end{verbatim}

This means there is one class per function.
From the interface we can also see that it uses
\textit{push/enter} function application model.
The argument stack is just a stack of \texttt{Object}s
which leads to inefficient boxing of primitive values.
A trampoline is used to apply functions with exception handling
as means of creating PAP objects.
This is probably due to a lack of tail calls in the early
versions of the .NET runtime.

Mondrian is no longer available in any shape or form
(even the projects web domain points to a different site).
However, the description of its implementation can show us
where optimizations can be done:

\begin{itemize}
    \item calling an interface method is costly, calling an
            overloaded method from a base class is cheaper;
    \item using \textit{push/enter} with boxing can be substituted
            by seperate stacks for primitive values (see below)
            or by using \textit{eval/apply} method.
\end{itemize}

\section{Haskell.NET}

In 2005 appeared the first project to bring Haskell
to the .NET platform.
Monique Monteiro et al. published a paper \cite{Brazil}
describing the implementation. This work also
uses predefined universal classes for thunks and functions and was
a primary motivator to do the same for this lazy runtime.

Haskell.NET also uses universal objects for data constructors.
Therefore all switching is done on their tags. This
is less efficient for small number of alternatives as shown
in \myref{perf:switching}.

For function application \textit{push/enter} method is used
with multiple stacks -- one for object references and
one for each primitve type. This removes costly boxing.
However, when evaluating a thunk
the current arguments have to be hidden.
The state of the stacks has to be saved and set as empty.
This simulates the fact that in GHC arguments and update frames share
the same stack and the thunk computation shouldn't have access to arguments
not meant for it.

The paper mentions a modified version of GHC
with a new backend for .NET compilation, which
unfortunately is no longer available.
Similarly to the compiler described in chapter \myref{r:compiler},
the modified GHC generated code from STG representation.
This way it utilizes a lot of GHC's optimizations and obtains
explicit closure definitions that are easy to translate.

However, it compiled Haskell straight to CIL rather than to C\shrp{}.
This results in more flexibility in the compiler and removes the
need to use a secondary compiler (especially a modified one).
The downside is the amount of work that has to be put into
creating this compiler and its optimizations.
Additionally, using C\shrp{} for Lazer cheaply allowed for error debugging
using existing tools.

\section{Oliver Hunt's work}

In 2006 Oliver Hunt published his Master's paper
on \textit{,,The Provision of Non-Strictness, Higher Kinded Types
and Higher Ranked Types on an Object Oriented
Virtual Machine''} \cite{Hunt}.
In this paper he provides a lot of theoretical details
on lazy functional programming and comparison with imperative programming.

Hunt puts more emphasis on strongly typed data constructors
in order to simplify interoperability.
This way a \verb|Cons<A>| has a head element of type \verb|A|
rather than \verb|Closure|. Therefore, the external user of the type
can easily know what was it supposed to return and can switch on
its variants. However, it may introduce additional monomorphisms,
i.e. a \verb|Nil<A>| is no longer generic for all \verb|A| and cannot be shared. 

Hunt describes potential support of Higher Kinded Types (HKT)
by the means of generic type arguments
with partial type erasure. Rather than resorting to
\textit{implementation leakage} (see \myref{s:data_rep})
in the compiled code, he uses casts to convert from an abstracted
(erased) type to a representable type (see chapter VII of his paper).
This of course is somewhat costly.

Higher Ranked Types are for example functions with a type parameter
that has a universally quantified variable, i.e:

\begin{verbatim}
    f :: (forall s. [s] -> [s]) -> ([a],[b]) -> ([a],[b])
    f g (x,y) = (g x, g y)\end{verbatim}

This means that generic type instantiation (here \verb|s|) has to be done on the
function application level rather than on the function's type.
Otherwise \verb|g| would only support one type - either \verb|a| or \verb|b|.
Hunt supports this and describes a solution that utilizes
generic methods as well as generic types (see chapter VIII of his paper).

While stronger typing has benefits in terms of readability and
interoperability of the compiled code, it adds complexity to the compiler
as well as increases the number of classes the runtime library has to provide.
In Hunt's approach there has to be a \verb|Thunk| class created for every
data type (e.g. \verb|ListThunk<A> : List<A>|), because the specific type is referenced rather
than a generic common superclass.

When it comes to function application Hunt removes the need
for either \textit{eval/apply} or \textit{push/enter} by
performing lambda lifting and explicit partial application
at compile time. Functions are implemented with classes
extending a set of common base classes for given arity.

Additionally Hunt presents that type classes
may be implemented as interfaces with instances implementing those interfaces.
The Lazer runtime uses the same unified approach as GHC
where instance dictionaries are compiled the same way
as data constructors and then for each entry in the dictionary
a selector function is created.

Hunt's paper also provides a description for an experimental
compiler. It uses GHC to compile Haskell to the Core
intermediate language which is then processed to produce C\shrp{}.
Unfortunately, again, access to this compiler wasn't possible.

\section{Comparison summary}

How is this runtime different from the previous solutions for the .NET platform?

\begin{enumerate}
    \item It focuses on performance. Although, a little bit of performance
            may be sacrificed for the sake of code readability.
    \item It uses \textit{eval/apply} function application strategy
            which fits very well an OOVM. Additionally, function application
            is heavily generic, which allows to use performance benefits of
            passing and returning primitive types the same way as closures,
            putting the runtime behaviour implementation details on the .NET JIT.
    \item It uses function pointers explicitly in C\shrp{}. While it's hard to
            say if Monteiro et al. used function pointers, an argument can be made
            that they did, because they emitted CIL directly and that is the most
            efficient way to do it.
    \item It uses type erasure, but not universal data containers. The interoperability
            is still limited, but properly named data constructors make it
            easier to see what's happening during debugging.
\end{enumerate}

However, the main motivation for this work was not to be different, but
to provide a complete working runtime to the reader. The classes defined
in \myref{s:runtime_impl} are the exact classes the runtime uses. Therefore,
even if some time passes and the runtime's source code disappears, it can be recreated
from this paper (except for the modified C\shrp{} compiler).

% ##########################
\chapter{Performance}\label{r:perf}
% ##########################

There are many consideration to be made when creating
an efficient runtime. Some performance issues are easy
to identify and others are extremely vague and dependent
on very specific details of how the CPU interacts with
our code and data.

This chapter presents some of the considered performance aspects.
Next, performance tests are presented that support the choices
made regarding switching, thunk indirections, and function pointers.
Finally, Lazer runtime and compiler are compared to the GHC by running
a few benchmarks.

\section{Performance considerations}

While chapter \myref{r:alternatives} mentioned a lot of similar
works, a lot of Lazer development happened before they've been
discovered. During those early phases of development the runtime
was very slow and with each iteration of work performance gains were
discovered.

One of the first mistakes was trying to explicitly check whether
the closure is already evaluated or if it should be entered.
Extensive branching may lead to an increase in branch
prediction fails which slows things down.
However, here the culprit was an inefficient type cast.
Removing the checks resulted in about 10\% performance increase.

Why was the type cast inefficient? As mentioned in \myref{s:compute_cond}
there is a fast type cast in .NET, but it only works for
sealed types. However, the sealed data constructors had a
base class that the cast was made to. And the base class
obviously could not be sealed.

The current implementation of the runtime uses universal
classes that perform indirect calls.
In fact every method call on an object is already an indirect call.
Too many indirections can be detrimental to the programs performance.
However, currently the compiler doesn't optimize those, because implementing
optimizations takes time.

\subsection{Garbage Collection}

In .NET there is a global GC that stops all threads
and performs the collection.
The stoppage may occur at any time when the program
runs out of free allocation space.
It is a generational tracing GC,
which means that short living objects are cheaper to collect.
Still, the less objects are allocated, the less time is
spent in GC and the program is faster.

The way the GC works is that it traverses the stack looking
for pointers. References between objects form a graph with
the stack being a root. If there isn't a path from an object
to the root, then this object can be collected.
A side effect of this implementation is that
non-tail recursive functions that grow the stack
make the traversal a bit more expensive.

When dealing with lots of short lived objects it may be
a good idea to implement some kind of pooling mechanism
to decrease the number of allocations and the frequency of
the GC process. The singleton pattern can be utilized
for argumentless data constructors and local functions
without free variables.

\subsection{Data structures}

During the development of the runtime a question was posed
whether heap based continuations would alleviate the problems
of limited stack space. The idea has been turned down
due to a performance slowdown.
However, with that implementation came an issue
of choosing the correct data structure for a continuation stack.
An array based stack proved to be faster then linked list of references.
But the most efficient approach was to form a linked chain
from the continuation objects themselves, by giving each a reference
to the next continuation.
This way much fewer allocation were made, while keeping
the O(1) cost of push and pop operations.

\section{Performance Tests}
It often happens that we perceive one way as faster then the other,
but in reality things are different. The best way to handle that
is to run performance tests, because those usually don't lie.
Below are three tests I felt necessary to present to support
claims I make in this paper, regarding the efficiency of the runtime.

\subsection{Switching on Tag vs Type}\label{perf:switching}

As explained in \myref{s:compute_cond} there are two ways
to switch on expressions, by type casting or by switching
on the constructor's tag. This test is meant to compare
performance of both. The following Haskell code has been implemented in C\shrp{}:

\begin{verbatim}
    data O = O0 Int# | O1 Int# | ... | O9 Int#
    
    extractO :: O -> Int
    extractO o =
        case o of
            O0 o0 -> I# o0
            O1 o1 -> I# o1
            ...
            O9 o9 -> I# o9

    suma !a []     = a
    suma !a (x:xs) = suma xs (a+x)

    take 0 _      = []
    take _ []     = []
    take n (x:xs) = x : take (n-1) xs

    loop :: O -> [O]
    loop o = o : loop o

    test o = suma 0 (take 500000 $ map extractO $ loop o)
\end{verbatim}


The test was ran on constant sequences of
\texttt{O0}, \texttt{O4} and \texttt{O9} to see
if there is a noticeable difference in run time
between linearly increasing time spent on switching on types
versus indirect jump on the tag in constant time.

\begin{center}
\begin{tabular}{c r c c c c}
    & & MIN & AVG & MAX & $\sigma$ \\
    \cline{3-6}
        
    \multirow{2}{*}{O0} & Switch Type & 164ms & 171ms & 189ms & 6.8 \\
    & Switch Tag & 167ms & 172ms & 186ms & 5.3 \\
    \cline{2-6}

    \multirow{2}{*}{O4} & Switch Type & 168ms & 173ms & 200ms & 7.2 \\
    & Switch Tag & 165ms & 173ms & 195ms & 8.5 \\
    \cline{2-6}

    \multirow{2}{*}{O9} & Switch Type & 170ms & 177ms & 201ms & 7.3 \\
    & Switch Tag & 169ms & 174ms & 185ms & 5.4 \\
\end{tabular}
\end{center}

The results of both types of switches are very close.
Switching on types is initially a little faster, however,
it does get slower with more alternatives.
The reason is that indirect jumps are more expensive than comparisons,
but the number of comparisons increases, while the cost of
an indirect jump and a cast is constant.
Switches with 5 or more alternatives should choose them by tag.

\subsection{Dynamic Thunk indirection call}\label{perf:thunk_ind}

In \myref{s:thunk_class} a mechanism has been shown were
on each call to \texttt{Eval} a comparison is made to
check if the indirection pointer is null.
This may seem inefficient and could potentially be implemented
by calling a ``dynamic'' method.
However, .NET objects reference their types' method tables
and cannot modify them. This forces the use of an indirect call.

\begin{verbatim}
    public unsafe abstract class Thunk : Computation {
        public Closure ind;
        private void* eval;
        protected Thunk()
            => eval = CLR.LoadFunctionPointer(PerformComputation);
        
        public override Closure Eval()
            => CLR.TailCallIndirectGeneric<Thunk, Closure>(this, eval);
        
        private Closure PerformComputation() {
            ind = Blackhole.Instance;
            ind = Compute();
            eval = CLR.LoadFunctionPointer(ReturnIndirection);
            Cleanup();
            return ind;
        }

        private Closure ReturnIndirection()
            => ind.Eval();
    }
\end{verbatim}

The following tests were performed on a slightly simplified
implementation, keeping the idea intact:

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{Create a million thunks}
    & Comparison & 0.3ms & 0.4ms & 0.7ms \\
    & Dynamic method & 7.7ms & 10.4ms & 14.4ms \\
    \cline{2-5}

    \multirow{2}{*}{Call \texttt{Eval} a million times} 
    & Comparison & 1ms & 1.2ms & 1.7ms \\
    & Dynamic method & 4.8ms & 5.2ms & 6.2ms \\
    
\end{tabular}
\end{center}

It turned out that the cost of an indirect call is much higher then the cost of comparison, not to mention the additional cost of assigning the dynamic method in the constructor.

\subsection{Function pointers vs Delegates}\label{perf:fun_pointers}

In .NET the legitimate way of passing functions around
are method delegates. The standard library provides
universal classes such as \texttt{Action} and \texttt{Func}
that are created from a function pointer and possibly
an additional reference to an object in case of instance methods.

A small test example has been created to measure the time
it takes to create a million delegates or pointers and then
measure the time it takes to invoke a single one a million times.

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{Create}
    & Delegate & 22ms & 23.4ms & 35ms \\
    & Pointer & 8.8ms & 9.4ms & 12.7ms \\
    \cline{2-5}
    
    \multirow{2}{*}{Invoke}
    & Delegate & 4.6ms & 4.8ms & 5.6ms \\
    & Pointer & 4ms & 4.3ms & 5.4ms \\
\end{tabular}
\end{center}

The results show that invocation takes approximately the same time
(there's one additional memory read for delegates per invocation),
but there's nearly a 3x difference in creation time.
This proves pointers to be a superior solution
when efficiency is required.

\section{Benchmarks}\label{s:benchmarks}

While the Lazer runtime is efficient in theory,
practical benchmarks needed to be ran in order to
compare its performance with GHC. If the speed difference
is too large than it's not so clear if other benefits
of bringing Haskell to .NET outweigh the performance loss.

\subsection{Running tests}

The following benchmarks are performed on three platforms:
the Lazer runtime described in this paper,
GHC interactive (via runhaskell),
GHC compiled.
Lazer programs were created by using the compiler described in
\myref{r:compiler}.
The GHC interactive runs are mostly the slowest ones,
thus they form a nice upper bound -- if Lazer was slower
then it is clearly inefficient.

In order to test the computation multiple times
within a single process instance, to circumvent
runtime initialization costs, the computations had to be
wrapped in a non-constant function. Otherwise,
the \textit{optimizing} compiler GHC replaced the
function call with a thunk and the test would only run once.

\subsection{Nofib: exp3\_8}

This test is from the nofib suite and is meant to test
the efficiency of arithmetic operations (addition and multiplication)
on natural numbers in unary form. Below are the Haskell source
code and test results:

\begin{verbatim}
    infix 8 ^^^

    data Nat = Z | S Nat deriving (Eq,Ord)
    instance Num Nat where
        Z   + y   = y
        S x + y   = S (x + y)
        x   * Z   = Z
        x   * S y = x * y + x
        fromInteger x = if x < 1 then Z else S (fromInteger (x-1))

    int :: Nat -> Int
    int Z     = 0
    int (S x) = 1 + int x

    x ^^^ Z   = S Z
    x ^^^ S y = x * (x ^^^ y)
\end{verbatim}

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{\texttt{int (3 \^{}\^{}\^{} 8)}}
    & GHC (compiled)& 0.160s & 0.163s & 0.168s \\
    & GHC (interpreted)& 1.87s & 1.89s & 1.91s \\
    & Lazer (JIT)& 1.11s & 1.15s & 1.47s \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{int (3 \^{}\^{}\^{} 9)}}
    & GHC (compiled)& 1.96s & 1.98s & 2.0s \\
    & GHC (interpreted)& 17.3s & 17.6s & 17.9s \\
    & Lazer (JIT)& 15.6s & 15.9s & 17.0s \\
\end{tabular}
\end{center}

We can see that Lazer is about 7-8x slower than compiled GHC.
It's highly likely to be due to a large number of allocations
and lengthy GC process. GHC's GC is optimized for a lazy
functional language, whereas .NET's GC is more general purpose.

\subsection{Nofib: digits of e}

This test is from the nofib suite and is meant to test
the efficiency of arithmetic operations 
on arbitrary precision integers.
The GHC \texttt{Integer} implementation uses GNU GMP
library. The .NET implementation uses \texttt{System.Numerics.BigInteger}.
Below are the Haskell source code and test results (function executed 100 times):

\begin{verbatim}
    type ContFrac = [Integer]

    eContFrac :: ContFrac
    eContFrac = 2:aux 2 where aux n = 1:n:1:aux (n+2)
    
    ratTrans :: (Integer,Integer,Integer,Integer) -> ContFrac -> ContFrac
    ratTrans (a,b,c,d) xs |
      ((signum c == signum d) || (abs c < abs d)) && -- No pole in range
      (c+d)*q <= a+b && (c+d)*q + (c+d) > a+b       -- Next digit is determined
         = q:ratTrans (c,d,a-q*c,b-q*d) xs
      where q = b `div` d
    ratTrans (a,b,c,d) (x:xs) = ratTrans (b,a+x*b,d,c+x*d) xs
    
    takeDigits :: Int -> ContFrac -> [Integer]
    takeDigits 0 _ = []
    takeDigits n (x:xs) = x:takeDigits (n-1) (ratTrans (10,0,0,1) xs)
    
    e :: Int -> [Integer]
    e n = takeDigits n eContFrac
\end{verbatim}

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{\texttt{e 50 $\cdot$ 100}}
    & GHC (compiled)& 0.36s & 0.35s & 0.38s \\
    & GHC (interpreted)& 1.90s & 1.91s & 1.96s \\
    & Lazer (JIT)& 0.53s & 0.54s & 0.56s \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{e 150 $\cdot$ 100}}
    & GHC (compiled)& 3.48s & 3.50s & 3.55s \\
    & GHC (interpreted)& 17.9s & 18.0s & 18.2s \\
    & Lazer (JIT)& 5.50s & 5.70s & 5.91s \\
\end{tabular}
\end{center}

Here Lazer is around 1.5x slower than compiled GHC.
There is a difference in the underlying representation
of arbitrary precision integers and their operations
implementations, which is definitely a factor.

\subsection{Nofib: primes}

This test is from the nofib suite and is meant to test
the efficiency of list operations 
(\texttt{map}, \texttt{filter}, \texttt{iterate}).
Below are the Haskell source code and test results (function executed 100 times).

\begin{verbatim}
    isdivs :: Int  -> Int -> Bool
    isdivs n x = mod x n /= 0
    
    the_filter :: [Int] -> [Int]
    the_filter (n:ns) = filter (isdivs n) ns
    
    prime :: Int -> Int
    prime n = map head (iterate the_filter [2..n*n]) !! n
\end{verbatim}

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{\texttt{primes 500 $\cdot$ 100}}
    & GHC (compiled)& 0.77s & 0.78s & 0.82s \\
    & GHC (interpreted)& 3.95s & 4.00s & 4.44s \\
    & Lazer (JIT)& 2.21s & 2.26s & 2.49s \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{primes 1500 $\cdot$ 100}}
    & GHC (compiled)& 4.0s & 4.0s & 4.1s \\
    & GHC (interpreted)& 37.9s & 38.1s & 38.4s \\
    & Lazer (JIT)& 25.4s & 25.7s & 27.3s \\
\end{tabular}
\end{center}

Here Lazer is 3$\sim$6x slower than compiled GHC.
Potential culprits are allocations and unknown function
application (see \myref{bench:app}).

\subsection{Sums}

In this test a few different approaches were applied to
obtain the sum of 100 thousand natural numbers.
Below is the Haskell source code and test results:

\begin{verbatim}
    sum [] = 0::Int
    sum (x:xs) = x + sum' xs

    suma [] !acc = acc::Int
    suma (x:xs) !acc = suma xs (x+acc)

    sumfold = foldl (+) (0::Int)
    foldl f x0 h = go x0 h
        where
            go x0 [] = x0
            go x0 (x:xs) = go (f x0 x) xs

    inf = 1 : map (+1) inf
\end{verbatim}

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{\texttt{sum (take 100000 inf)}}
    & GHC (compiled)& 2.27ms & 3.36ms & 7.54ms \\
    & GHC (interpreted)& 94.6ms & 106ms & 144ms \\
    & Lazer (JIT)& 2.55ms & 3.54ms & 5.73ms \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{suma (take 100000 inf) 0}}
    & GHC (compiled)& 1.06ms & 1.18ms & 2.08ms \\
    & GHC (interpreted)& 75.2ms & 76.9ms & 84.0ms \\
    & Lazer (JIT)& 2.08ms & 4.55ms & 10.6ms \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{sumfold (take 100000 inf)}}
    & GHC (compiled)& 1.08ms & 1.23ms & 2.44ms \\
    & GHC (interpreted)& 59.5ms & 71.1ms & 100.8ms \\
    & Lazer (JIT)& 2.09ms & 2.90ms & 5.10ms \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{sum [1..100000]}}
    & GHC (compiled)& 2.06ms & 2.97ms & 6.47ms \\
    & GHC (interpreted)& 40.7ms & 52.5ms & 105ms \\
    & Lazer (JIT)& 1.42ms & 1.86ms & 3.88ms \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{suma [1..100000] 0}}
    & GHC (compiled)& 0.84ms & 0.95ms & 1.28ms \\
    & GHC (interpreted)& 31.4ms & 32.6ms & 37.2ms \\
    & Lazer (JIT)& 0.94ms & 1.30ms & 2.70ms \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{sumfold [1..100000]}}
    & GHC (compiled)& 0.60ms & 0.76ms & 2.65ms \\
    & GHC (interpreted)& 15.0ms & 27.3ms & 63.8ms \\
    & Lazer (JIT)& 0.95ms & 1.29ms & 2.55ms \\
\end{tabular}
\end{center}

The addition operation in those tests is executed
eagerly. Thanks to strictness analysis special worker
functions are created, which resamble imperative loops.
Lazer is mostly within the 1-2x slower range with functions
that use \texttt{take} going up to 2-4x slower.

\subsection{Tight arithmetic loops}

Both GHC and Lazer show a speed up when performing
operations eagerly for tight arithmetic loops.
Below are two loops in Haskell and test results:

\begin{verbatim}
    fiba :: Int -> Int -> Int -> Int
    fiba !a !b n = if n <= 0 then a else fiba b (a+b) (n-1)
    
    fibt = fiba 0 1

    sumFromTo :: Int -> Int -> Int
    sumFromTo from to = go from to 0
        where go !f !t !n | f > t     = n
                          | otherwise = go (f+1) t (n+f)
\end{verbatim}

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{\texttt{fibt 800000}}
    & GHC (compiled)& 0.85ms & 0.90ms & 1.01ms \\
    & GHC (interpreted)& 645ms & 659ms & 709ms \\
    & Lazer (JIT)& 0.68ms & 0.78ms & 1.06ms \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{sumFromTo 1 100000}}
    & GHC (compiled)& 0.061ms & 0.063ms & 0.097ms \\
    & GHC (interpreted)& 81ms & 83ms & 92ms \\
    & Lazer (JIT)& 0.057ms & 0.066ms & 0.25ms \\
\end{tabular}
\end{center}

In those tight loops Lazer performs just as well as GHC
or even a little faster.

\subsection{Uknown function application}\label{bench:app}

This test measures the cost of calling \texttt{Apply}
on an unknown function.

\begin{verbatim}
    loopApp :: (Int -> Int) -> Int -> Int -> Int
    loopApp f w 0 = f 0 + w
    loopApp f w n = loopApp f (f n + w) (n-1)
    
    loopCall :: Int-> Int-> Int
    loopCall w 0 = add1 0 + w
    loopCall w n = loopCall (w + add1 n) (n-1)
    
    {-# NOINLINE add1 #-}
    add1 :: Int -> Int
    add1 x = x+1
\end{verbatim}

\begin{center}
\begin{tabular}{c r c c c}
    & & MIN & AVG & MAX \\
    \cline{3-5}

    \multirow{2}{*}{\texttt{loopApp add1 0 100000}}
    & GHC (compiled)& 0.88ms & 0.94ms & 1.19ms \\
    & GHC (interpreted)& 79.4ms & 95.6ms & 129ms \\
    & Lazer (JIT)& 3.84ms & 4.30ms & 7.84ms \\
    \cline{2-5}

    \multirow{2}{*}{\texttt{loopCall 0 100000}}
    & GHC (compiled)& 0.460ms & 0.511ms & 0.823ms \\
    & GHC (interpreted)& 71.4ms & 94.3ms & 125ms \\
    & Lazer (JIT)& 0.062ms & 0.081ms & 0.109ms \\
\end{tabular}
\end{center}

For compiled GHC unknown function application is at most
twice as slow as known function application.
For Lazer its over 50x slower. The .NET runtime may
apply additional checks on the object and the invoked
method to assert type safety. Even without the checks
we're still looking at
an indirect call and 3-4 indirect jumps, which is
much slower than a single direct call.

Why is Lazer faster than GHC when using direct calls?
This probably comes from
the fact that GHC checks stack depth on function entry,
whereas .NET uses \textit{Segmentation fault} system interrupt
to handle stack overflows.
This is because GHC can use potentially infinite stack space
and has to allocate more space when it runs out.

\section{Summary}

The benchmarks clearly show that Lazer runtime is performing
best when it's doing strict evaluation.
Unknown functions application and large amounts of object allocations
are the major causes of slowdown.
Some of the slowdowns can be fixed by making a better compiler,
others are unfortunately bound to the runtime limitations.

Overall, the performance is decent and the runtime can be used
for example with graphical user interfaces that don't require
extremely low latency. The processing parts that do
require high performance have to implemented eagerly
or reference external .NET libraries.

% ##########################
\chapter{Compiling Haskell to C\shrp{}}\label{r:compiler}
% ##########################

\section{GHC compiler library}

\section{Translating STG}

\section{Optimizations}

% ##########################
\chapter{Future work}\label{r:future}
% ##########################

TODO
% blackholes with locks


\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliography}

\bibitem[MT-96]{Tullsen} Mark Tullsen, \textit{,,Compiling Haskell to Java''}, YALEU/DCS/RR-1204
1996.

\bibitem[BA-07]{Alliet} Brian Alliet, \textit{,,Efficient Translation of Haskell to Java
Master’s Thesis Proposal''}, 2007.

\bibitem[KC,HL,TH-01]{Choi} Kwanghoon Choi, Hyun-il Lim, Taisook Han, \textit{,,Compiling Lazy Functional Programs Based othe Spineless Tagless G-machine for the Java Virtual Machine''}, FLOPS 2001.

\bibitem[DS-02]{Stewart} Don Stewart, \textit{,,Multi-paradigm just-in-time compilation''}, 2002.

\bibitem[NP,EM-04]{PerryMeijer} Nigel Perry, Erik Meijer, \textit{,,Implementing functional languages on object-oriented virtual machines''}, IEE Proceedings on Software, 151(1):1-9, 2004.

\bibitem[OH-06]{Hunt} Oliver Hunt, \textit{,,The Provision of Non-Strictness, Higher Kinded Types
and Higher Ranked Types on an Object Oriented Virtual Machine''}, 2006.

\bibitem[EM,NP,AY-01]{MondrianImplDetails} Erik Meijer, Nigel Perry, Arjan van Yzendoorn, \textit{,,Scripting .NET using Mondrian''}, ECOOP 2001.

\bibitem[MM,MA,RB,AS-05]{Brazil} Monique Monteiro, Mauro Ara\'ujo, Rafael Borges, Andr\'e Santos,  \textit{,,Compiling Non-strict Functional Languages for the .NET Platform''}, Journal of Universal Computer Science, vol. 11, no. 7 (2005), 1255-1274.

\bibitem[HS,PS-90]{referentialTransparency} Harald Søndergaard, Peter Sestoft, \textit{,,Referential Transparency, Definiteness and Unfoldability''}, Acta Informatica 27, 505-517, 1990.

\bibitem[VH,SB,TG-19]{lazySpark} Val\'erie Hayot-Sasson, Shawn T Brown, Tristan Glatard, \textit{,,Performance Evaluation of Big Data Processing Strategies for Neuroimaging''}, CCGRID, 2019.

\bibitem[CO-96]{amortized} Chris Okasaki, \textit{,,The Role of Lazy Evaluation in Amortized Data Structures''} ICFP'96, 62-72, 1996.

\bibitem[SPJ-87]{spj-book} Simon Peyton Jones, \textit{,,The Implementation of Functional Programming Languages''}, 1987.

\bibitem[PL-64]{SECDM} P.J. Landin, \textit{,,The Mechanical Evaluation of Expressions''}, Computer Journal, Vol 6, No 4, 1964.

\bibitem[AD,DM-90]{CASE} Anthony Davie, David McNally, \textit{,,CASE -- A Lazy Version of an SECD Machine with a Flat Environment''}, 1990.

\bibitem[RK-85]{G-Machine} Richard B. Kieburtz, \textit{,,The G-machine: a fast, graph-reduction evaluator''}, Technical Report CS/E-85-002, Dept. of Computer Science, Oregon Graduate Center, January 1985.

\bibitem[DT-79]{combinators} David A. Turner, \textit{,,A new implementation technique for applicative languages''}, Software -- Practice and Experience, 9:31–49, 1979.

\bibitem[TC,PG,CM,AN-80]{SKIM} T.J.W. Clarke, P.J.S. Gladstone, C.D. MacLean, A.C. Norman, \textit{,,SKIM -- The S, K, I Reduction Machine''}, \texttt{Proceedings 1980 LISP conference}, pages 128-135, 1980.

\bibitem[MM,AS-86]{CAM} Michel Mauny, Asc\'ander Su\'arez, \textit{,,Implementing functional languages in the categorical abstract machine''}, In \texttt{Proceedings 1986 ACM Conference on Lisp and Functional Programming}, pages 266–278, Cambridge, Massachusetts, August 1986.

\bibitem[PC-86]{categorical_combinators} P.-L. Curien, \textit{,,Categorical Combinators''}, 1986.

\bibitem[JF,SW-87]{TIM} Jon Fairbairn, Stuart Wray, \textit{,,Tim: a simple, lazy abstract machine to execute supercombinators''},  In \texttt{Proceedings of 1987 Functional Programming Languages and Computer Architecture Conference}, pages 34–45, Springer Verlag LNCS 274, September 1987.

\bibitem[SPJ-92]{STGM} Simon Peyton Jones, \textit{,,Implementing lazy functional languages on stock hardware: the Spineless Tagless G-machine''}, 1992.

\bibitem[IS,SPJ,DV-17]{demand analysis} Ilya Sergey, Simon Peyton Jones, Dimitrios Vytiniotis, \textit{,,Theory and Practice of Demand Analysis in Haskell''}, 2017.

\bibitem[HC-36]{Curry} Haskell Curry, \textit{,,Functionality in combinatory logic''}, \texttt{Proceedings
of the National Academy of Sciences (U.S.A.)}, XX, pp. 584—590, 1936.

\bibitem[SM,SPJ-04]{fastcurry} Simon Marlow, Simon Peyton Jones, \textit{,,Making a fast Curry: Push/Enter vs. Eval/Apply for Higher-order Languages''} ICFP'04, pp. 4-15, 2004.

\bibitem[TH,SM,SPJ-05]{multiprocessor} Tim Harris, Simon Marlow, Simon Peyton Jones, \textit{,,Haskell on a Shared-Memory Multiprocessor''}, 2005.

\bibitem[PL-02]{coq_extract} Pierre Letouzey, \textit{,,A New Extraction for Coq''}, TYPES, 2002.

\end{thebibliography}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
